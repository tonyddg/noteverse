import{_ as c}from"./plugin-vue_export-helper-c27b6911.js";import{r as i,o as p,c as r,b as n,d as s,a,w as l,e as t}from"./app-8c5ce49e.js";const d={},u=t('<h1 id="神经网络" tabindex="-1"><a class="header-anchor" href="#神经网络" aria-hidden="true">#</a> 神经网络</h1><p>主要 Pytorch 介绍的使用</p><p>笔记中的代码默认导入模块</p><ul><li>Pytorch 模块 <code>import torch</code></li><li>Numpy 模块 <code>import numpy as np</code></li></ul><h2 id="神经网络基础" tabindex="-1"><a class="header-anchor" href="#神经网络基础" aria-hidden="true">#</a> 神经网络基础</h2>',5),m={href:"https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html",target:"_blank",rel:"noopener noreferrer"},h=n("h3",{id:"张量类型",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#张量类型","aria-hidden":"true"},"#"),s(" 张量类型")],-1),k=n("br",null,null,-1),b=n("a",{href:"#%E8%87%AA%E5%8A%A8%E6%A2%AF%E5%BA%A6"},"自动梯度",-1),v=n("p",null,[s("以下使用 "),n("code",null,"tensor"),s(" 表示任意张量对象")],-1),g=n("h4",{id:"创建张量",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#创建张量","aria-hidden":"true"},"#"),s(" 创建张量")],-1),_=n("code",null,"torch.tensor(data, dtype = torch.float32)",-1),y=n("code",null,"data",-1),f=n("li",null,[n("code",null,"dtype"),s(" 元素的类型, 类似 Numpy 数组")],-1),E=n("li",null,[n("code",null,"torch.from_numpy(np_array)"),s(" 通过 Numpy 数组创建张量 "),n("ul",null,[n("li",null,[n("code",null,"np_array"),s(" 为 Numpy 数组对象")]),n("li",null,[s("注意, 该方法创建的张量将与传入的 Numpy 数组共享同一块内存空间, 即该方法创建的张量对象被修改时, 也将反映到传入的 Numpy 数组对象 "),n("code",null,"np_array"),s(" 中")])])],-1),x=n("li",null,[n("code",null,"torch.rand(shape, dtype)"),s(" 创建随机张量, 元素取值为 "),n("span",{class:"katex"},[n("span",{class:"katex-mathml"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("semantics",null,[n("mrow",null,[n("mo",{stretchy:"false"},"("),n("mn",null,"0"),n("mo",{separator:"true"},","),n("mn",null,"1"),n("mo",{stretchy:"false"},")")]),n("annotation",{encoding:"application/x-tex"},"(0,1)")])])]),n("span",{class:"katex-html","aria-hidden":"true"},[n("span",{class:"base"},[n("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),n("span",{class:"mopen"},"("),n("span",{class:"mord"},"0"),n("span",{class:"mpunct"},","),n("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),n("span",{class:"mord"},"1"),n("span",{class:"mclose"},")")])])]),s(", 类型为 "),n("code",null,"torch.float32"),n("ul",null,[n("li",null,[n("code",null,"shape"),s(" 元组, 表示创建张量的大小")]),n("li",null,[n("code",null,"dtype"),s(" 元素的类型")])])],-1),B=n("li",null,[n("code",null,"torch.ones(shape)"),s(" 创建使用 "),n("span",{class:"katex"},[n("span",{class:"katex-mathml"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("semantics",null,[n("mrow",null,[n("mn",null,"1")]),n("annotation",{encoding:"application/x-tex"},"1")])])]),n("span",{class:"katex-html","aria-hidden":"true"},[n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.6444em"}}),n("span",{class:"mord"},"1")])])]),s(" 填充的张量, 参数含义同上")],-1),A=n("li",null,[n("code",null,"torch.zeros(shape)"),s(" 创建使用 "),n("span",{class:"katex"},[n("span",{class:"katex-mathml"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("semantics",null,[n("mrow",null,[n("mn",null,"0")]),n("annotation",{encoding:"application/x-tex"},"0")])])]),n("span",{class:"katex-html","aria-hidden":"true"},[n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.6444em"}}),n("span",{class:"mord"},"0")])])]),s(" 填充的张量, 参数含义同上")],-1),w=n("h4",{id:"张量的方法",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#张量的方法","aria-hidden":"true"},"#"),s(" 张量的方法")],-1),z=t('<li>使用以下方法可对张量的属性进行查询 <ul><li><code>tensor.shape</code> 元组, 表示张量各维度的大小</li><li><code>tensor.dtype</code> 张量的元素类型</li></ul></li><li>张量合并与堆叠 <ul><li>使用成员函数 <code>torch.cat(tensors, dim)</code> 合并张量 (不能增加维度) <ul><li><code>tensors</code> 由参与合并张量组成的元组</li><li><code>dim</code> 合并方向的维度</li><li>使用 <code>torch.cat(tensors, 2)</code> 可将输入的 <code>k</code> 个 <code>n x m x 1</code> 的张量堆叠为 <code>n x m x k</code></li></ul></li><li>使用成员函数 <code>torch.stack(tensors, dim)</code> 堆叠张量 (用于增加维度) <ul><li><code>tensors</code> 由参与堆叠张量组成的元组</li><li><code>dim</code> 堆叠方向的维度</li><li>使用 <code>torch.stack(tensors, 1)</code> 可将输入的 <code>k</code> 个 <code>n x m</code> 的张量堆叠为 <code>n x m x k</code></li></ul></li><li>使用成员函数 <code>tensor.view(*shapes)</code> 可修改张量形状 (获取于原有张量数据相同但形状不同的张量) <ul><li><code>shapes</code> 张量各个维度的大小, 传入 <code>-1</code> 表示由原有形状自动确定</li><li>一般通过 <code>tensor.view(1, -1)</code> 令张量升维</li></ul></li></ul></li><li>张量的操作硬件 <ul><li><code>tensor.device</code> 查询张量的操作硬件, 默认为 <code>cpu</code></li><li><code>tensor.to(device)</code> 将张量的操作硬件转换为指定的硬件</li><li>参考<a href="#%E6%A8%A1%E5%9E%8B%E8%BF%90%E8%A1%8C%E8%AE%BE%E5%A4%87">模型运行设备</a>的代码可用于自动检测当前环境可使用的最优硬件</li></ul></li>',3),q=n("li",null,[n("code",null,"tensor.numpy()"),s(" 将张量转换为 Numpy 数组, 两者共享内存")],-1),C=n("li",null,[n("code",null,"tensor.item()"),s(" 单个元素的张量转为数值, 用于处理损失函数的返回值")],-1),N=n("li",null,[s("与 Numpy 数组类似, 张量的一般运算为元素间匀速, 而 "),n("code",null,"@"),s(" 则为矩阵乘法")],-1),L={href:"https://pytorch.org/docs/stable/tensors.html#torch.Tensor",target:"_blank",rel:"noopener noreferrer"},T=n("li",null,[s("除了一般的操作, 张量类型还具有一类带有 "),n("code",null,"_"),s(" 后缀的方法, 此来方法为原地操作的版本, 可以节省内存空间, 但注意原始数据将永久修改, 一般仅在模型内使用"),n("br"),s(" 如果要在运算中使用, 则要调用方法, 如 "),n("code",null,"tensor.add_(x)")],-1),W=n("h4",{id:"自动梯度",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#自动梯度","aria-hidden":"true"},"#"),s(" 自动梯度")],-1),S=n("p",null,"Pytorch 中张量对象最大的特点即自动梯度, 通过自动梯度只需要使用张量对象的运算方法进行正向传播, 即可自动计算正向传播结果相对各个参数张量的梯度",-1),D=t("<li><code>tensor.requires_grad_(requires_grad = True)</code> 开启或关闭张量对象的自动梯度 <ul><li>默认创建张量对象时关闭自动梯度</li><li>在模型中, 当开启自动梯度时, 相当于将该张量的值视为训练参数</li><li>当关闭时相当于视为不变的常量, 或者用于加快训练完成模型的正向传播的速度</li></ul></li><li><code>tensor.backward()</code> 反向传播 <ul><li>当该函数被调用时, 将计算得到该张量对象 <code>tensor</code> 时, 经过所有自动梯度张量相对该张量 <code>tensor</code> 的梯度</li><li>注意 <code>tensor</code> 应使用一系列来自模块 <code>torch</code> 的函数 (如交叉熵损失函数 <code>torch.nn.functional.binary_cross_entropy_with_logits(z, y)</code>) 或张量对象间运算 (相加或矩阵乘等) 得到</li><li>通常一个运算产生的张量对象只能进行一次反向传播</li></ul></li><li><code>tensor.requires_grad</code> 布尔型对象成员变量, 为 <code>True</code> 时表示该张量具有自动梯度</li><li><code>tensor.grad</code> 张量型对象成员变量 <ul><li>该张量对象在最近一次参与运算后, 反向传播得到的梯度 (需要最终结果调用 <code>tensor.backward()</code>)</li><li>如果该张量之前没有进行过有效的反向传播, 则该变量的值为 <code>None</code></li><li>如果该张量已经有梯度了, 再次进行反向传播将叠加上原有梯度, 如果要避免叠加可对该变量重新赋 <code>None</code></li></ul></li>",4),M=n("code",null,"with torch.no_grad():",-1),F=n("br",null,null,-1),P=t(`<h3 id="构建模型" tabindex="-1"><a class="header-anchor" href="#构建模型" aria-hidden="true">#</a> 构建模型</h3><p>构建模型时需要导入模块 <code>from torch import nn</code></p><h4 id="定义模型对象" tabindex="-1"><a class="header-anchor" href="#定义模型对象" aria-hidden="true">#</a> 定义模型对象</h4><p>对于模型对象通常需要继承模型基类 <code>nn.Module</code><br> 此时仅需要定义类的各层网络与正向传播过程, 该基类将据此提供控制模型的有关函数<br> 一般 Pytorch 的张量具有<a href="#%E8%87%AA%E5%8A%A8%E6%A2%AF%E5%BA%A6">自动梯度</a>的功能, 因此不需要专门定义反向传播函数</p><p>通常定义模型的代码如下</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">NeuralNetwork</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>flatten <span class="token operator">=</span> nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear_relu_stack <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">28</span><span class="token operator">*</span><span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        logits <span class="token operator">=</span> self<span class="token punctuation">.</span>linear_relu_stack<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> logits
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>在模型对象的构造函数中 <ul><li>需要首先调用模型基类的构造函数 <code>super().__init__()</code></li><li>定义模型的各层网络 <ul><li>一般使用对象的成员变量保存网络层对象, 成员变量名即层的名称</li><li>对于中间隐藏层与输出, 一般使用 <code>nn.Sequential</code> 作为容器包裹将其统一为一层网络</li></ul></li></ul></li><li>在模型对象的正向传播函数中 <ul><li>正向传播函数接收一个参数, 即模型的输入</li><li>以函数的方式调用各层网络对象, 将数据输入, 并将输出的中间结果作为下层的输入, 最后的输出即正向传播结果</li></ul></li></ul><h4 id="常用网络层介绍" tabindex="-1"><a class="header-anchor" href="#常用网络层介绍" aria-hidden="true">#</a> 常用网络层介绍</h4>`,8),O=t(`<p>神经网络中的各层通过模块 <code>nn</code> 下的网络层类创建<br> 网络层类的实例为可调用的对象, 能够接收张量进行运算并输出</p><p>在定义网络中, 一般将样本的最高维视为批次, 例如 <code>B x L</code> 的张量将被视为一个具有 <code>B</code> 个批次的一维的张量</p><ul><li><code>nn.Flatten(start_dim = 1, end_dim = -1)</code> 张量展开层 <ul><li><code>start_dim, end_dim</code> 将输入张量从 <code>start_dim</code> 到 <code>end_dim</code> 的维度展开</li><li>默认将输入的 <code>B x A1 x A2 x ...</code> 的张量展开为 <code>B x L</code> 的张量</li><li>通常可作为输入层将输入的图像转为一维序列</li></ul></li><li><code>nn.Linear(in_features, out_features)</code> 全连接神经元层 <ul><li><code>in_features</code> 输入元素数</li><li><code>out_features</code> 输出元素数</li><li>即一般的线性全连接神经元, 但不包含激活函数, 要求输入张量 <code>B x I</code> 输出张量 <code>B x O</code></li><li>通常下一层应为激活函数层, 并且参数保存在激活函数中</li></ul></li><li>激活函数层 (根据上一层的全连接神经元层自动确定规模) <ul><li><code>nn.ReLU()</code> ReLU 激活函数层</li><li><code>nn.Sigmoid()</code> Sigmoid 激活函数层</li></ul></li><li><code>nn.Softmax(dim)</code> Softmax 归一化输出激活函数层 <ul><li><code>dim</code> 归一化的维度, 对于 <code>B x L</code> 的输入一般取 <code>1</code></li><li>一般作为输出层, 将输入张量的最高维分别使用 SoftMax 函数归一化, 保证输出张量的归一化维度上元素之和为 1 (如输出概率质量函数)<br> 一般输入 <code>B x L</code> 的张量, 将各通道下的一维数组的元素归一化</li><li>对于 Softmax 的预测结果, 可使用 <code>tensor.argmax(1)</code> 提取各批次结果中, 概率最大的元素索引</li><li>该网络层同样为激活函数, 根据上一层的全连接神经元层自动确定规模</li></ul></li><li><code>nn.Sequential(*args)</code> 网络顺序容器 <ul><li><code>args</code> 从输入到输出的各层子网络层对象</li></ul></li></ul><h4 id="模型运行设备" tabindex="-1"><a class="header-anchor" href="#模型运行设备" aria-hidden="true">#</a> 模型运行设备</h4><p>对于构建完成的模型, 需要在示例化后使用方法 <code>model.to(device)</code> 选择模型使用的硬件</p><p>推荐使用以下代码获取当前环境可用的最优硬件</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>device <span class="token operator">=</span> <span class="token punctuation">(</span>
    <span class="token string">&quot;cuda&quot;</span>
    <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">else</span> <span class="token string">&quot;mps&quot;</span>
    <span class="token keyword">if</span> torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>mps<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">else</span> <span class="token string">&quot;cpu&quot;</span>
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="模型基本使用" tabindex="-1"><a class="header-anchor" href="#模型基本使用" aria-hidden="true">#</a> 模型基本使用</h4><ul><li>将模型转换为字符串或直接打印模型对象, 可输出模型各层网络的信息</li><li>通过模型对象方法 <code>model.named_parameters()</code> 可返回一个可迭代对象, 每个迭代元素为一个 <code>(name, param)</code> 的元组 <ul><li><code>name</code> 模型中具有训练参数的网络层中, 所带有的参数部分名称, 如 ReLu 层有权重矩阵与偏差两个带有参数的部分<br> 名称一般为 <code>&lt;层名称&gt;.&lt;子层序号&gt;.&lt;参数部分名&gt;</code><ul><li><code>层名称</code> 即<a href="#%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%B1%A1">定义模型对象</a>时保存层对象的成员变量</li><li><code>子层序号</code> 见打印的模型各层网络的信息</li><li><code>参数部分名</code> 与层类型有关, 如</li></ul></li><li><code>param</code> 该部分的参数张量, 通过 <code>param.shape</code> 可以查询参数各维度的大小</li><li>可通过循环 <code>for name, param in model.named_parameters():</code> 遍历</li></ul></li><li>模型对象为可调用对象, 通过传入样本可直接输出训练结果完成正向传播 <ul><li>根据<a href="#%E5%B8%B8%E7%94%A8%E7%BD%91%E7%BB%9C%E5%B1%82%E4%BB%8B%E7%BB%8D">样本的特性</a>, 即使仅有一个样本要预测也应当传入 <code>1 x L</code> 的张量<br> 可利用 <code>tensor.view(1, -1)</code> 进行升维, 或 <code>tensor.stack(...)</code> 进行堆叠, 详见<a href="#%E5%BC%A0%E9%87%8F%E7%9A%84%E6%96%B9%E6%B3%95">张量的方法</a></li><li>一般网络参数的张量类型为 <code>torch.float32</code>, 因此也要保证传入的样本张量类型为 <code>torch.float32</code><br> 可在转换为张量时明确元素类型属性 <code>dtype</code></li><li>关于模型预测代码详见<a href="#%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B">模型预测</a></li></ul></li><li>模型对像方法 <code>model.train()</code> 将模型设置为训练模式</li><li>模型对像方法 <code>model.eval()</code> 将模型设置为预测模式</li></ul><h3 id="模型训练" tabindex="-1"><a class="header-anchor" href="#模型训练" aria-hidden="true">#</a> 模型训练</h3><h4 id="模型训练器" tabindex="-1"><a class="header-anchor" href="#模型训练器" aria-hidden="true">#</a> 模型训练器</h4><p>模型训练器 (模型优化器) 类定义于模块 <code>torch.optim</code> 下, 通过模型训练器可以完成模型的反向传播过程</p>`,12),H=t('<li>传统梯度优化器 <code>torch.optim.SGD(params, lr = 0.001)</code><ul><li><code>params</code> 模型参数迭代器, 通过模型对象的方法 <code>model.parameters()</code> 得到</li><li><code>lr</code> 模型的学习率</li><li>通过修改<a href="#%E5%8D%95%E6%AC%A1%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83">单次模型训练</a>的细节, 可实现多种传统梯度下降算法</li></ul></li>',1),I={href:"https://pytorch.org/docs/stable/optim.html",target:"_blank",rel:"noopener noreferrer"},R=t("<li>由于 <code>SGD</code> 无法解决局部最优问题, 因此实际场景中最常用的是 <ul><li>优化器 <code>torch.optim.Adam</code> 及其变种 <code>torch.optim.NAdam</code> 以获得最快的训练速度, 但可能不稳定</li><li>优化器 <code>torch.optim.SGD(params, lr = 0.001, momentum = 0.9)</code> 带有动量的 SGD 以获得最好的训练效果 (需要在小模型上调参, 参数 <code>momentum</code> 为动量系数)</li></ul></li>",1),G={href:"https://zhuanlan.zhihu.com/p/416979875",target:"_blank",rel:"noopener noreferrer"},U=t('<p>无论使用何种优化器, 一般均通过以下两个对象方法进行一次反向传播</p><ul><li><code>optimizer.step()</code> 根据正向传播的<a href="#%E8%87%AA%E5%8A%A8%E6%A2%AF%E5%BA%A6">预测结果反向传播</a>得到的自动梯度更新模型中的参数</li><li><code>optimizer.zero_grad()</code> 清空模型中的梯度, 防止影响下次反向传播 (<a href="#%E8%87%AA%E5%8A%A8%E6%A2%AF%E5%BA%A6">自动梯度</a>中不会覆盖之前反向传播的梯度)</li></ul><h4 id="训练相关超参数" tabindex="-1"><a class="header-anchor" href="#训练相关超参数" aria-hidden="true">#</a> 训练相关超参数</h4><p>在单次训练前, 首先确定以下超参数</p>',4),X=t('<li>训练循环次数 <code>epoch</code><ul><li>通过<a href="#%E5%8D%95%E6%AC%A1%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83">单次模型训练</a>循环的循环次数控制</li></ul></li><li>数据批次大小 <code>batch_size</code><ul><li>通过<a href="#%E8%AE%BF%E9%97%AE%E6%95%B0%E6%8D%AE%E9%9B%86">访问数据集</a>中 <code>Dataloader</code> 类构造函数的参数指定</li><li>体现为传入样本的最高维维度</li></ul></li><li>学习率 <code>lr</code><ul><li>一般取值为 <code>1e-3</code></li><li>如果使用 Sigmoid 激活函数, 一般则取值为 <code>0.1</code></li></ul></li>',3),j=n("code",null,"loss_fn",-1),V=n("code",null,"torch.nn",-1),J={href:"https://pytorch.org/docs/stable/nn.html#loss-functions",target:"_blank",rel:"noopener noreferrer"},K=t("<li>常用的损失函数定义类有交叉熵 <code>nn.CrossEntropyLoss</code> (用于分类问题), 均方差 <code>nn.MSELoss</code> (用于回归问题) 等</li><li>通过 <code>loss_fn = nn.CrossEntropyLoss()</code> 即可实例化损失函数 <ul><li>损失函数对象可接收参数 <code>loss = loss_fn(pred, y)</code>, 无论传入张量大小总是<mark>返回一个单元素张量表示该批次训练的损失之和</mark> (使用 <code>tensor.item()</code> 可转为 <code>float</code> 类型)</li><li><code>pred</code> 为模型预测结果, <code>y</code> 为标记的结果, 传入的参数类型必须是张量 (因此导入数据集时必须数据预处理为张量, 即使只有一个值)</li><li>当传入的 <code>y</code> 为单元素时, 将自动转换为独热编码, 因此一般不需要将 <code>y</code> 预处理为独热编码</li><li>允许传入一个批次 (batch) 的数据, 此时 <code>pred</code> 的大小为 <code>B x L</code>, <code>y</code> 的大小为 <code>B</code>, 首先将 <code>y</code> 转换为独热编码, 再计算逐个批次的损失并返回平均值</li><li>对返回的张量调用 <code>loss.backward()</code> 获取参数梯度</li></ul></li>",2),Q=t(`<h4 id="单次模型训练" tabindex="-1"><a class="header-anchor" href="#单次模型训练" aria-hidden="true">#</a> 单次模型训练</h4><p>以下为单次训练函数示例</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">train_loop</span><span class="token punctuation">(</span>dataloader<span class="token punctuation">,</span> model<span class="token punctuation">,</span> loss_fn<span class="token punctuation">,</span> optimizer<span class="token punctuation">)</span><span class="token punctuation">:</span>
    size <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>dataloader<span class="token punctuation">)</span>

    <span class="token comment"># 将模型设置为训练模式, 每次新的训练循环开始前必须调用此函数</span>
    model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">for</span> batch<span class="token punctuation">,</span> <span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>dataloader<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 通过传入样本进行正向传播</span>
        pred <span class="token operator">=</span> model<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
        <span class="token comment"># 将正向传播结果传入损失函数, 与正确结果对比</span>
        loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> y<span class="token punctuation">)</span>

        <span class="token comment"># 调用正向传播结果张量的 backward 方法进行反向传播, 自动计算梯度</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        <span class="token comment"># 根据自动梯度, 使用优化器更新模型参数</span>
        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 将梯度归零</span>
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token comment"># 每 100 个 batch 打印模型训练进度</span>
        <span class="token keyword">if</span> batch <span class="token operator">%</span> <span class="token number">100</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            <span class="token comment"># loss 单元素张量, 通过 item 方法转为浮点数</span>
            loss <span class="token operator">=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;loss: </span><span class="token interpolation"><span class="token punctuation">{</span>loss<span class="token punctuation">:</span><span class="token format-spec">&gt;7f</span><span class="token punctuation">}</span></span><span class="token string">  [</span><span class="token interpolation"><span class="token punctuation">{</span>batch<span class="token punctuation">:</span><span class="token format-spec">&gt;5d</span><span class="token punctuation">}</span></span><span class="token string">/</span><span class="token interpolation"><span class="token punctuation">{</span>size<span class="token punctuation">:</span><span class="token format-spec">&gt;5d</span><span class="token punctuation">}</span></span><span class="token string">]&quot;</span></span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="模型测试" tabindex="-1"><a class="header-anchor" href="#模型测试" aria-hidden="true">#</a> 模型测试</h4><p>以下为模型正确率测试函数示例, 通常在一个训练循环完成后进行一次测试, 以绘制出模型的训练曲线</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">test_loop</span><span class="token punctuation">(</span>dataloader<span class="token punctuation">,</span> model<span class="token punctuation">,</span> loss_fn<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 将模型设置为预测模式, 当模型用于预测前必须调用此函数</span>
    model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

    size <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>dataloader<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span>
    num_batches <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>dataloader<span class="token punctuation">)</span>
    test_loss<span class="token punctuation">,</span> correct <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span>

    <span class="token comment"># 在模型预测时关闭自动梯度以获得更好的运算速度</span>
    <span class="token comment"># 并且防止模型预测干扰之后的训练</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 通常测试集的数据同样时按 batch 传入</span>
        <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> dataloader<span class="token punctuation">:</span>
            pred <span class="token operator">=</span> model<span class="token punctuation">(</span>X<span class="token punctuation">)</span>

            <span class="token comment"># 累加预测损失, 函数的返回值即该 batch 的平均损失</span>
            test_loss <span class="token operator">+=</span> loss_fn<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
            
            <span class="token comment"># 累加预测正确率</span>
            <span class="token comment"># pred.argmax(1) 将最大概率的结果作为预测分类, 注意做高维度 0 为 batch</span>
            <span class="token comment"># .type(torch.float).sum().item 先将布尔值转为浮点数, 再累加整个 batch 的预测准确率, 最后从张量转为浮点数</span>
            correct <span class="token operator">+=</span> <span class="token punctuation">(</span>pred<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">==</span> y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># 计算每次预测的平均损失</span>
    test_loss <span class="token operator">/=</span> num_batches
    <span class="token comment"># 计算每次预测的平均准确率</span>
    correct <span class="token operator">/=</span> size
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Test Error: \\n Accuracy: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token operator">*</span>correct<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">&gt;0.1f</span><span class="token punctuation">}</span></span><span class="token string">%, Avg loss: </span><span class="token interpolation"><span class="token punctuation">{</span>test_loss<span class="token punctuation">:</span><span class="token format-spec">&gt;8f</span><span class="token punctuation">}</span></span><span class="token string"> \\n&quot;</span></span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="一般模型训练" tabindex="-1"><a class="header-anchor" href="#一般模型训练" aria-hidden="true">#</a> 一般模型训练</h4><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader
<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> datasets
<span class="token keyword">from</span> torchvision<span class="token punctuation">.</span>transforms <span class="token keyword">import</span> ToTensor

<span class="token comment"># 定义模型类</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>

<span class="token comment"># 加载训练集</span>
training_data <span class="token operator">=</span> datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span>
    root<span class="token operator">=</span><span class="token string">&quot;data&quot;</span><span class="token punctuation">,</span>
    train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    transform<span class="token operator">=</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span>

<span class="token comment"># 加载测试集</span>
test_data <span class="token operator">=</span> datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span>
    root<span class="token operator">=</span><span class="token string">&quot;data&quot;</span><span class="token punctuation">,</span>
    train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
    download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    transform<span class="token operator">=</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span>

<span class="token comment"># 确定训练相关的超参数</span>
learning_rate <span class="token operator">=</span> <span class="token number">1e-3</span>
batch_size <span class="token operator">=</span> <span class="token number">64</span>
epochs <span class="token operator">=</span> <span class="token number">5</span>
loss_fn <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 根据超参数示例化优化器, 数据加载器</span>
optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>learning_rate<span class="token punctuation">)</span>
train_dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>training_data<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">)</span>
test_dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>test_data<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">)</span>

<span class="token comment"># 进行训练</span>
<span class="token keyword">for</span> t <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Epoch </span><span class="token interpolation"><span class="token punctuation">{</span>t<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">\\n-------------------------------&quot;</span></span><span class="token punctuation">)</span>
    train_loop<span class="token punctuation">(</span>train_dataloader<span class="token punctuation">,</span> model<span class="token punctuation">,</span> loss_fn<span class="token punctuation">,</span> optimizer<span class="token punctuation">)</span>
    test_loop<span class="token punctuation">(</span>test_dataloader<span class="token punctuation">,</span> model<span class="token punctuation">,</span> loss_fn<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;Done!&quot;</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="模型保存与加载" tabindex="-1"><a class="header-anchor" href="#模型保存与加载" aria-hidden="true">#</a> 模型保存与加载</h3><h4 id="官方预训练模型" tabindex="-1"><a class="header-anchor" href="#官方预训练模型" aria-hidden="true">#</a> 官方预训练模型</h4><p>通过模块 <code>import torchvision.models as models</code> 可以导入主流的模型类以及这些模型的预训练参数<br> 可以直接基于这些主流模型类训练自己的模型, 或者直接导入预训练参数使用模型</p>`,11),Y={href:"https://pytorch.org/vision/stable/models.html",target:"_blank",rel:"noopener noreferrer"},Z=n("br",null,null,-1),$=n("a",{href:"#%E6%A8%A1%E5%9E%8B%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8"},"模型基本使用",-1),nn=t('<p>以下以深度卷积网络 <code>vgg16</code> 为例</p><ul><li><code>model = models.vgg16(weights)</code> 创建 <code>vgg16</code> 模型对象 <ul><li><code>weights</code> 预训练参数一般为一个字符串, 具体见文档</li></ul></li></ul><h4 id="保存模型" tabindex="-1"><a class="header-anchor" href="#保存模型" aria-hidden="true">#</a> 保存模型</h4><ul><li><code>torch.save(obj, f)</code> 保存模型 <ul><li><code>obj</code> 保存的数据 <ul><li>传入模型对象 <code>model</code> 时, 将保存整个模型</li><li>传入 <code>model.state_dict()</code> 时, 仅保存模型内的参数</li></ul></li><li><code>f</code> 保存路径或文件对象, 一般带有后缀 <code>.pth</code></li></ul></li></ul><h4 id="读取模型" tabindex="-1"><a class="header-anchor" href="#读取模型" aria-hidden="true">#</a> 读取模型</h4><ul><li><code>torch.load(f)</code> 读取模型 <ul><li><code>f</code> 保存路径或文件对象</li><li>当保存整个模型时将返回模型对象</li><li>当保存模型参数时, 仅有模型参数信息</li></ul></li><li><code>model.load_state_dict(data)</code> 读取参数信息 <ul><li><code>data</code> 通过 <code>torch.load(f)</code> 读取到的参数信息</li><li>注意 <code>model</code> 必须与保存参数的模型来自同一个模型类</li></ul></li></ul><h2 id="常见神经网络构建" tabindex="-1"><a class="header-anchor" href="#常见神经网络构建" aria-hidden="true">#</a> 常见神经网络构建</h2><h3 id="图像识别" tabindex="-1"><a class="header-anchor" href="#图像识别" aria-hidden="true">#</a> 图像识别</h3><p>对于图像样本一般视为 <code>B x C x W x H</code> 的张量, 其中</p><ul><li><code>B</code> 为批次即 batch</li><li><code>C</code> 为通道, 即使时灰度图也视为具有通道 <code>C = 1</code></li><li><code>W, H</code> 图片的宽与高</li></ul><h4 id="卷积网络层" tabindex="-1"><a class="header-anchor" href="#卷积网络层" aria-hidden="true">#</a> 卷积网络层</h4>',11),sn=n("ul",null,[n("li",null,[n("code",null,"nn.Conv2d(in_channels, out_channels, kernel_size, stride = 1, padding = 0)"),s(" 一般卷积网络层 "),n("ul",null,[n("li",null,[n("code",null,"in_channels, out_channels"),s(" 输入与输出的图像通道数")]),n("li",null,[n("code",null,"kernel_size"),s(" 整数 "),n("code",null,"k"),s(", 卷积核大小为 "),n("code",null,"k x k")]),n("li",null,[n("code",null,"stride"),s(" 卷积核移动步长")]),n("li",null,[n("code",null,"padding"),s(" 填充边缘")]),n("li",null,[s("对于输入 "),n("span",{class:"katex"},[n("span",{class:"katex-mathml"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("semantics",null,[n("mrow",null,[n("mi",null,"C"),n("mo",null,"×"),n("mi",null,"W"),n("mo",null,"×"),n("mi",null,"W")]),n("annotation",{encoding:"application/x-tex"},"C \\times W \\times W")])])]),n("span",{class:"katex-html","aria-hidden":"true"},[n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.7667em","vertical-align":"-0.0833em"}}),n("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"C"),n("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),n("span",{class:"mbin"},"×"),n("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.7667em","vertical-align":"-0.0833em"}}),n("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"W"),n("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),n("span",{class:"mbin"},"×"),n("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.6833em"}}),n("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"W")])])]),s(", 输出图像大小变为 "),n("span",{class:"katex"},[n("span",{class:"katex-mathml"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("semantics",null,[n("mrow",null,[n("mi",null,"W"),n("mo",null,"="),n("mi",null,"f"),n("mi",null,"l"),n("mi",null,"o"),n("mi",null,"o"),n("mi",null,"r"),n("mo",{stretchy:"false"},"("),n("mfrac",null,[n("mrow",null,[n("mi",null,"W"),n("mo",null,"−"),n("mi",null,"k"),n("mo",null,"+"),n("mn",null,"2"),n("mi",null,"p")]),n("mi",null,"s")]),n("mo",{stretchy:"false"},")"),n("mo",null,"+"),n("mn",null,"1")]),n("annotation",{encoding:"application/x-tex"},"W = floor(\\frac{W - k + 2p}{s})+1")])])]),n("span",{class:"katex-html","aria-hidden":"true"},[n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.6833em"}}),n("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"W"),n("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),n("span",{class:"mrel"},"="),n("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),n("span",{class:"base"},[n("span",{class:"strut",style:{height:"1.2772em","vertical-align":"-0.345em"}}),n("span",{class:"mord mathnormal",style:{"margin-right":"0.10764em"}},"f"),n("span",{class:"mord mathnormal",style:{"margin-right":"0.01968em"}},"l"),n("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"oor"),n("span",{class:"mopen"},"("),n("span",{class:"mord"},[n("span",{class:"mopen nulldelimiter"}),n("span",{class:"mfrac"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.9322em"}},[n("span",{style:{top:"-2.655em"}},[n("span",{class:"pstrut",style:{height:"3em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight"},"s")])])]),n("span",{style:{top:"-3.23em"}},[n("span",{class:"pstrut",style:{height:"3em"}}),n("span",{class:"frac-line",style:{"border-bottom-width":"0.04em"}})]),n("span",{style:{top:"-3.4461em"}},[n("span",{class:"pstrut",style:{height:"3em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.13889em"}},"W"),n("span",{class:"mbin mtight"},"−"),n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03148em"}},"k"),n("span",{class:"mbin mtight"},"+"),n("span",{class:"mord mtight"},"2"),n("span",{class:"mord mathnormal mtight"},"p")])])])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.345em"}},[n("span")])])])]),n("span",{class:"mclose nulldelimiter"})]),n("span",{class:"mclose"},")"),n("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),n("span",{class:"mbin"},"+"),n("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.6444em"}}),n("span",{class:"mord"},"1")])])])]),n("li",null,[s("注意, 该卷积网络层不具备激活函数, 因此下一层需要为"),n("a",{href:"#%E5%B8%B8%E7%94%A8%E7%BD%91%E7%BB%9C%E5%B1%82%E4%BB%8B%E7%BB%8D"},"激活函数层")])])]),n("li",null,[n("code",null,"nn.MaxPool2d(kernel_size, stride = None)"),s(" 最大池化层 "),n("ul",null,[n("li",null,[n("code",null,"kernel_size"),s(" 整数 "),n("code",null,"k"),s(", 卷积核大小为 "),n("code",null,"k x k")]),n("li",null,[n("code",null,"stride"),s(" 卷积核移动步长, 默认与卷积核相同")]),n("li",null,[s("对于输入 "),n("span",{class:"katex"},[n("span",{class:"katex-mathml"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("semantics",null,[n("mrow",null,[n("mi",null,"C"),n("mo",null,"×"),n("mi",null,"W"),n("mo",null,"×"),n("mi",null,"W")]),n("annotation",{encoding:"application/x-tex"},"C \\times W \\times W")])])]),n("span",{class:"katex-html","aria-hidden":"true"},[n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.7667em","vertical-align":"-0.0833em"}}),n("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"C"),n("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),n("span",{class:"mbin"},"×"),n("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.7667em","vertical-align":"-0.0833em"}}),n("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"W"),n("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),n("span",{class:"mbin"},"×"),n("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.6833em"}}),n("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"W")])])]),s(", 输出图像大小变为 "),n("span",{class:"katex"},[n("span",{class:"katex-mathml"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("semantics",null,[n("mrow",null,[n("mi",null,"W"),n("mo",null,"="),n("mi",null,"f"),n("mi",null,"l"),n("mi",null,"o"),n("mi",null,"o"),n("mi",null,"r"),n("mo",{stretchy:"false"},"("),n("mfrac",null,[n("mrow",null,[n("mi",null,"W"),n("mo",null,"−"),n("mi",null,"k")]),n("mi",null,"k")]),n("mo",{stretchy:"false"},")"),n("mo",null,"+"),n("mn",null,"1")]),n("annotation",{encoding:"application/x-tex"},"W = floor(\\frac{W - k}{k})+1")])])]),n("span",{class:"katex-html","aria-hidden":"true"},[n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.6833em"}}),n("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"W"),n("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),n("span",{class:"mrel"},"="),n("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),n("span",{class:"base"},[n("span",{class:"strut",style:{height:"1.2251em","vertical-align":"-0.345em"}}),n("span",{class:"mord mathnormal",style:{"margin-right":"0.10764em"}},"f"),n("span",{class:"mord mathnormal",style:{"margin-right":"0.01968em"}},"l"),n("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"oor"),n("span",{class:"mopen"},"("),n("span",{class:"mord"},[n("span",{class:"mopen nulldelimiter"}),n("span",{class:"mfrac"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.8801em"}},[n("span",{style:{top:"-2.655em"}},[n("span",{class:"pstrut",style:{height:"3em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03148em"}},"k")])])]),n("span",{style:{top:"-3.23em"}},[n("span",{class:"pstrut",style:{height:"3em"}}),n("span",{class:"frac-line",style:{"border-bottom-width":"0.04em"}})]),n("span",{style:{top:"-3.394em"}},[n("span",{class:"pstrut",style:{height:"3em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.13889em"}},"W"),n("span",{class:"mbin mtight"},"−"),n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03148em"}},"k")])])])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.345em"}},[n("span")])])])]),n("span",{class:"mclose nulldelimiter"})]),n("span",{class:"mclose"},")"),n("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),n("span",{class:"mbin"},"+"),n("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.6444em"}}),n("span",{class:"mord"},"1")])])])])])]),n("li",null,[n("code",null,"nn.Flatten(start_dim = 1, end_dim = -1)"),s(" 将图像转换为一维特征向量, 详见"),n("a",{href:"#%E5%B8%B8%E7%94%A8%E7%BD%91%E7%BB%9C%E5%B1%82%E4%BB%8B%E7%BB%8D"},"之前介绍")])],-1),an=n("h2",{id:"训练数据",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#训练数据","aria-hidden":"true"},"#"),s(" 训练数据")],-1),en=n("br",null,null,-1),tn={href:"https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#preparing-your-data-for-training-with-dataloaders",target:"_blank",rel:"noopener noreferrer"},on=n("p",null,[s("使用来自 Pytorch 的训练数据时, 需要安装模块 "),n("code",null,"torchvision")],-1),ln={href:"https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files",target:"_blank",rel:"noopener noreferrer"},cn=n("h3",{id:"加载数据集",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#加载数据集","aria-hidden":"true"},"#"),s(" 加载数据集")],-1),pn=n("p",null,[s("首先导入公开数据集加载模块 "),n("code",null,"from torchvision import datasets")],-1),rn=n("code",null,"torchvision.datasets",-1),dn=n("br",null,null,-1),un={href:"https://pytorch.org/vision/stable/datasets.html",target:"_blank",rel:"noopener noreferrer"},mn=t(`<p>以公开数据集 <code>FashionMNIST</code> 的加载为例<br><code>xxx_data = torchvision.datasets.FashionMNIST(*, root, train = True, download = False, transform = None, target_transform = None)</code></p><ul><li><code>root</code> 公开数据集的本地保存路径</li><li><code>train</code> 是否提取其中的训练集, 传入 <code>False</code> 则提取测试集</li><li><code>download</code> 传入 <code>True</code> 时, 当数据集不在本地时, 尝试下载并保存在 <code>root</code> 指定的路径下</li><li><code>transform</code> 加载原始特征数据时的预处理器, 默认即数据集的原始数据类型 更多见<a href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86">数据预处理</a></li><li><code>target_transform</code> 加载标签数据时的预处理器, 默认即数据集的原始数据类型, 更多见<a href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86">数据预处理</a></li></ul><h3 id="访问数据集" tabindex="-1"><a class="header-anchor" href="#访问数据集" aria-hidden="true">#</a> 访问数据集</h3><p>在 Pytorch 中的数据集由多条数据组成, 每条数据仅包含特征 (即相同大小的原始数据, 一般为图像) 与标签 (根据数据集特性而定)<br> 通常有以下方法访问数据集</p><ul><li>直接访问数据集 <code>img, label = xxx_data[n]</code><ul><li><code>n</code> 访问数据集的第 <code>n</code> 条数据</li><li><code>img, label</code> 第 <code>n</code> 条数据的特征与标签</li></ul></li><li>通过 Pytorch 数据加载器访问, 一般用于传入模型中使用<br> 数据加载器通过 <code>from torch.utils.data import DataLoader</code> 导入 <ul><li>创建加载器对象<code>xxx_dataloader = Dataloader(dataset, batch_size = 1, shuffle = False)</code><ul><li><code>dataset</code> 被加载的数据集对象</li><li><code>batch_size</code> 数据集中一个批次 (batch) 的大小</li><li><code>shuffle</code> 是否每个轮次 (epoch) 都重新选择批次中的数据</li></ul></li><li>数据加载器与数据集类似, 但将原始数据额外添加一个最高为度, 大小即 <code>batch_size</code></li><li>迭代加载器对象一般代码 <code>for batch, (X, y) in enumerate(dataloader):</code> 可以得到迭代的批次数与批次数据</li></ul></li></ul><h3 id="数据预处理" tabindex="-1"><a class="header-anchor" href="#数据预处理" aria-hidden="true">#</a> 数据预处理</h3><p>在<a href="#%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86">加载数据集</a>时, 参数 <code>transform</code> 与 <code>target_transform</code> 可用于对数据集的特征与标签进行预处理<br> 预处理器为 <code>torchvision.transforms</code> 下的一类对象, 通过模块内的类实例化即可创建对应的预处理器</p><p>以下为常用的预处理器</p><ul><li><code>ToTensor()</code> 将图片等原始数据转换为 <code>C x W x H</code> 的张量</li><li><code>Lambda(lambda)</code> 将函数 <code>lambda</code> 包裹为预处理器处理原始数据, 并讲函数返回值作为处理后数据</li><li><code>Normalize(mean, std)</code> 将输入的 <code>C x W x H</code> 张量按特定规则标准化 <ul><li><code>mean, std</code> 为浮点数元组, 元组的低 <code>i</code> 个元素表示通道 <code>i</code> 的平均值与标准差</li></ul></li><li><code>Compose(transforms)</code> 组合排列一系列预处理器 <ul><li><code>transforms</code> 预处理器实例的元组, 将按顺序使用元组中的预处理器</li></ul></li></ul><p>以下为预处理器的使用实例</p><p>将数字标签预处理为 One-Hot 编码的例子</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> torchvision<span class="token punctuation">.</span>transforms <span class="token keyword">import</span> Lambda
<span class="token comment"># 创建预处理器, 该预处理器将 0-9 的标签转为 10 元素的 One-Hot 编码的一维张量</span>
<span class="token comment"># 其中 tensor.scatter_(...) 为张量的广播赋值函数</span>
one_hot_transforms <span class="token operator">=</span> Lambda<span class="token punctuation">(</span><span class="token keyword">lambda</span> y<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>scatter_<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">,</span> value<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>标准化图片像素的预处理器</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span>
    <span class="token punctuation">[</span>
        transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="高效训练" tabindex="-1"><a class="header-anchor" href="#高效训练" aria-hidden="true">#</a> 高效训练</h2><h3 id="模型详细信息-pytorch-summary" tabindex="-1"><a class="header-anchor" href="#模型详细信息-pytorch-summary" aria-hidden="true">#</a> 模型详细信息 Pytorch Summary</h3>`,16),hn={href:"https://github.com/sksq96/pytorch-summary",target:"_blank",rel:"noopener noreferrer"},kn=n("h3",{id:"训练监控-tensorboard",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#训练监控-tensorboard","aria-hidden":"true"},"#"),s(" 训练监控 TensorBoard")],-1),bn={href:"https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html",target:"_blank",rel:"noopener noreferrer"},vn={href:"https://zhuanlan.zhihu.com/p/471198169",target:"_blank",rel:"noopener noreferrer"},gn={href:"https://pytorch.org/docs/stable/tensorboard.html",target:"_blank",rel:"noopener noreferrer"},_n=t(`<p>通过 TensorBoard, 可以实现对模型训练过程的学习曲线与相关信息的记录以及远程访问</p><p>使用 TensorBoard 前需要安装模块 <code>pip install tensorboard</code><br> 并通过 <code>import torch.utils.tensorboard</code> 导入有关模块</p><h4 id="学习数据记录" tabindex="-1"><a class="header-anchor" href="#学习数据记录" aria-hidden="true">#</a> 学习数据记录</h4><p>在记录数据前, 首先需要使用 <code>from torch.utils.tensorboard.writer import SummaryWriter</code> 导入记录器类</p><p>数据记录器 <code>SummaryWriter</code></p><ul><li>通过 <code>from torch.utils.tensorboard.writer import SummaryWriter</code> 导入</li><li>Tensorboard 中, 通常使用一个工作目录保存所有学习数据, 目录下的子文件夹则为单次记录</li><li>构造函数 <code>writer = SummaryWriter(log_dir = None, comment = &quot;&quot;)</code><ul><li><code>log_dir</code> 学习数据记录路径, 默认为 <code>./runs/当前时间与用户信息</code>, 即将 <code>./runs</code> 作为工作目录</li><li><code>comment</code> 学习数据标记, 将添加到路径后</li></ul></li></ul><p>通过 <code>SummaryWriter</code> 对象的方法记录学习数据</p><ul><li>记录图表时, 可使用标签 <code>xxx/yyy</code>, 此时图表 <code>yyy</code> 将被归类到 <code>xxx</code> 中, 通常通过训练模型特性作为 <code>xxx</code></li><li>对于同一工作文件夹下的多个记录之间可以互通, 因此最好保证图表的标签是唯一的</li></ul><p>以下为 <code>SummaryWriter</code> 对象的常用方法</p><p>方法 <code>close()</code> 关闭学习记录器</p><p>方法 <code>add_scalars(main_tag, tag_scalar_dict, global_step = None)</code> 插入一组数据点</p><ul><li><code>main_tag</code> 数据图表标签</li><li><code>tag_scalar_dict</code> 插入数据点组, 为一个字典, 字典的键为特定折线名称, 值为该折线在此处的值</li><li><code>global_step</code> 数据点索引, 默认将数据点插入末尾</li><li>在 TensorBoard 中, 将体现为多线折线图, 可用于比较</li></ul><p>方法 <code>add_graph(model, input_to_model)</code> 插入模型详细信息图表</p><ul><li><code>model</code> 被插入的<a href="#%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B%E5%AF%B9%E8%B1%A1">模型对象</a></li><li><code>input_to_model</code> 用于测试的张量, 可通过 <a href="#%E5%88%9B%E5%BB%BA%E5%BC%A0%E9%87%8F">torch.rand</a> 创建一个符合模型输入要求的张量</li><li>该方法可创建一个关于模型内部详细信息的图表, 可用于分析模型各层规模以及层间交互</li></ul><p>方法 <code>add_hparams(hparam_dict, metric_dict)</code> 记录超参数模型表现信息</p><ul><li><code>hparam_dict</code> 一个以超参数名为键, 超参数值为值的字典, 记录该模型使用的超参数</li><li><code>metric_dict</code> 一个以表现类型为键, 该超参数下的模型表现为值的字典, 记录该模型的表现</li><li>该函数目前可能存在问题</li></ul><p>例如以下代码可用于监控模型训练的学习曲线</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>self<span class="token punctuation">.</span>writer<span class="token punctuation">.</span>add_scalars<span class="token punctuation">(</span>
    main_tag  <span class="token operator">=</span> <span class="token builtin">dir</span> <span class="token operator">+</span> <span class="token string">&quot;loss&quot;</span><span class="token punctuation">,</span> 
    tag_scalar_dict  <span class="token operator">=</span> <span class="token punctuation">{</span>
        <span class="token string">&quot;train&quot;</span><span class="token punctuation">:</span> train_loss<span class="token punctuation">,</span>
        <span class="token string">&quot;test&quot;</span><span class="token punctuation">:</span> test_loss
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    global_step <span class="token operator">=</span> t<span class="token punctuation">)</span>

self<span class="token punctuation">.</span>writer<span class="token punctuation">.</span>add_scalars<span class="token punctuation">(</span>
    main_tag  <span class="token operator">=</span> <span class="token builtin">dir</span> <span class="token operator">+</span> <span class="token string">&quot;correct&quot;</span><span class="token punctuation">,</span> 
    tag_scalar_dict  <span class="token operator">=</span> <span class="token punctuation">{</span>
        <span class="token string">&quot;train&quot;</span><span class="token punctuation">:</span> train_correct<span class="token punctuation">,</span>
        <span class="token string">&quot;test&quot;</span><span class="token punctuation">:</span> test_correct
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    global_step <span class="token operator">=</span> t<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="访问仪表盘" tabindex="-1"><a class="header-anchor" href="#访问仪表盘" aria-hidden="true">#</a> 访问仪表盘</h4><p>使用命令 <code>tensorboard --logdir=runs</code> 启动仪表盘服务器</p><ul><li>参数 <code>--logdir</code> 即指定工作文件夹位置</li><li>当服务器启动成功时, 将输出访问网页地址</li><li>可以在模型训练时实时监控</li><li>对于远程访问等方法参见上文链接教程</li></ul><p>查看数据折线图</p><ul><li>如果要查看数据折线图, 需要选择 <code>SCALARS</code> 选项卡</li><li>建议查看一般折线图时, 将左侧参数 <code>Smoothing</code> 设置为 <code>0</code>, 该参数用于平滑剧烈变化的曲线</li></ul><h4 id="其他数据记录方法" tabindex="-1"><a class="header-anchor" href="#其他数据记录方法" aria-hidden="true">#</a> 其他数据记录方法</h4><p>以下为 <code>SummaryWriter</code> 对象的其他记录学习数据方法</p><p>方法 <code>add_scalar(tag, scalar_value, global_step = None)</code> 插入单个数据点</p><ul><li><code>tag</code> 数据图表标签</li><li><code>scalar_value</code> 插入新数据点, 通常只能传入浮点数</li><li><code>global_step</code> 数据点索引, 默认将数据点插入末尾</li><li>在 TensorBoard 中, 将体现为折线图</li></ul><p>方法 <code>add_figure(tag, figure, close = True)</code> 插入 Matplotlib 图像</p><ul><li><code>tag</code> 图片标签</li><li><code>figure</code> 被插入的 Matplotlib 图像对象</li><li><code>close</code> 是否在插入后自动关闭 <code>figure</code> 对象</li></ul><p>方法 <code>add_image(tag, img_tensor)</code> 插入图片</p><ul><li><code>tag</code> 图片标签</li><li><code>img_tensor</code> 为 <code>(C,H,W)</code> 形状的张量表示视频, 可使用函数 <code>torchvision.utils.make_grid()</code> 分解 <code>(B,C,H,W)</code> 的张量</li><li>使用时要求安装模块 <code>pillow</code></li></ul><p>方法 <code>add_video(tag, vid_tensor, fps = 4)</code> 插入视频</p><ul><li><code>tag</code> 视频标签</li><li><code>vid_tensor</code> 为 <code>(N,T,C,H,W)</code> 形状的张量表示视频, 其中 <code>N</code> 表示第几个视频</li><li><code>fps</code> 视频帧率</li><li>使用时要求安装模块 <code>moviepy</code></li></ul><p>方法 <code>add_text(tag, text_string)</code> 插入文字</p><ul><li><code>tag</code> 文字标签</li><li><code>text_string</code> 插入的文字, 可用于对分类进行详细说明</li></ul><h3 id="性能分析-tensorboard-profiler" tabindex="-1"><a class="header-anchor" href="#性能分析-tensorboard-profiler" aria-hidden="true">#</a> 性能分析 TensorBoard Profiler</h3>`,36),yn={href:"https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html",target:"_blank",rel:"noopener noreferrer"},fn=n("h3",{id:"超参数调节-optuna",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#超参数调节-optuna","aria-hidden":"true"},"#"),s(" 超参数调节 Optuna")],-1),En={href:"https://optuna.readthedocs.io/en/stable/tutorial/index.html",target:"_blank",rel:"noopener noreferrer"},xn=t(`<p>通过 Optuna, 可以实现对模型超参数的快速搜索, 且 Optuna 与具体框架无关</p><p>使用 Optuna 前需要安装模块 <code>pip install optuna</code><br> 并通过 <code>import optuna</code> 导入有关模块</p><h4 id="基本搜索框架" tabindex="-1"><a class="header-anchor" href="#基本搜索框架" aria-hidden="true">#</a> 基本搜索框架</h4><p>Optuna 将超参数调节视为一种优化问题</p><ul><li>优化问题使用一个传入 <code>optuna.trial.Trial</code> 对象参数的函数<br> 在函数内通过与 <code>optuna.trial.Trial</code> 对象交互实现超参数的获取, 报告训练进程等</li><li>使用 <code>optuna.study.Study</code> 对象管理优化问题<br> 将优化问题传入 <code>optuna.study.Study</code> 对象从而对最优的一组超参数进行搜索, 并在搜索结束后获取结果</li></ul><p>因此 Optuna 的基本使用框架为</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> optuna

<span class="token comment"># 将优化问题定义为函数</span>
<span class="token keyword">def</span> <span class="token function">objective</span><span class="token punctuation">(</span>trial<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 通过向 trial 对象申请参数值, 隐性地定义了待调节的超参数</span>
    x <span class="token operator">=</span> trial<span class="token punctuation">.</span>suggest_float<span class="token punctuation">(</span><span class="token string">&quot;x&quot;</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
    <span class="token comment"># 将优化目标作为函数返回值传出, 通常即模型的准确率等评价指标</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span>x <span class="token operator">-</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span>

<span class="token comment"># 创建 optuna.study.Study 对象</span>
study <span class="token operator">=</span> optuna<span class="token punctuation">.</span>create_study<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># 搜索问题的最优超参数</span>
study<span class="token punctuation">.</span>optimize<span class="token punctuation">(</span>objective<span class="token punctuation">,</span> n_trials<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span>
<span class="token comment"># 搜索结束后, 获取最优超参数</span>
best_params <span class="token operator">=</span> study<span class="token punctuation">.</span>best_params
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="优化目标设置" tabindex="-1"><a class="header-anchor" href="#优化目标设置" aria-hidden="true">#</a> 优化目标设置</h4><p>如<a href="#%E5%9F%BA%E6%9C%AC%E6%90%9C%E7%B4%A2%E6%A1%86%E6%9E%B6">基本搜索框架</a>介绍, 在优化问题函数内, 通过 <code>optuna.trial.Trial</code> 对象注册超参数<br> Optuna 支持浮点, 整数, 类别三种超参数, 通过注册方法的返回值可获取其搜索值, 注册方法如下</p><ul><li>方法 <code>suggest_float(name, low, high, *, log = False)</code> 注册浮点型超参数 <ul><li><code>name</code> 字符串, 超参数名称</li><li><code>low, high</code> 浮点数, 超参数上下界</li><li><code>log</code> 是否使用 <code>log</code> 规律搜索参数</li></ul></li><li>方法 <code>suggest_int(name, low, high)</code> 注册整数超参数 <ul><li><code>name</code> 字符串, 超参数名称</li><li><code>low, high</code> 整数, 超参数上下界</li></ul></li><li>方法 <code>suggest_categorical(name, choices)</code> 注册类型超参数 <ul><li><code>name</code> 字符串, 超参数名称</li><li><code>choices</code> 列表, 类型列表, 超参数将从中采样</li></ul></li></ul><p>Optuna 的超参数可以灵活使用</p><ul><li>允许每次优化任务中, 由于 <code>if</code> 等语句, 而使用不同的超参数</li><li>允许通过循环创建多个超参数</li><li>建议不要创建过多的超参数, 否则将增加搜索复杂度</li></ul>`,12),Bn=n("br",null,null,-1),An={href:"https://optuna.readthedocs.io/zh-cn/stable/tutorial/10_key_features/003_efficient_optimization_algorithms.html",target:"_blank",rel:"noopener noreferrer"},wn=t(`<h4 id="优化过程管理" tabindex="-1"><a class="header-anchor" href="#优化过程管理" aria-hidden="true">#</a> 优化过程管理</h4><p>模块方法 <code>study = optuna.create_study(*, study_name = None, direction = None, storage = None, load_if_exists = False)</code> 创建优化任务管理对象</p><ul><li><code>study_name</code> 字符串, 学习名称</li><li><code>direction</code> 字符串, 优化方向, 可使用 <code>minimize</code> 或 <code>maximize</code> 表示希望最大化或最小化优化目标 (默认为最小化, 但常用的是最大化, 因此需要设置此参数)</li><li><code>storage</code> 管理数据保存的数据库地址 <ul><li>字符串, 一般即 <code>sqlite:///&lt;path&gt;</code>, <code>path</code> 为指向 <code>.db</code> 文件的相对路径, 不存在时将自动创建</li><li><code>None</code>, 管理的数据将保存在内存中, 将在程序结束后释放 (不建议)</li></ul></li><li><code>load_if_exists</code> 布尔值, 开启时如果管理数据存在, 则加载 <ul><li>通过设置该参数可以加载之前的训练, 从而在已有数据基础上继续训练或读取之前训练结果, 训练可视化等</li><li>如果已有同名且同数据库的数据存在, 且设置为 <code>False</code>, 将出错</li></ul></li><li>此处仅介绍部分参数, 其他参数参见具体功能介绍</li></ul><p>通过 <code>study</code> 对象可管理优化任务</p><ul><li>方法 <code>optimize(func, n_trials = None)</code> 创建优化任务并进行优化 <ul><li><code>func</code> 函数, 接收一个 <code>optuna.trial.Trial</code> 类型的参数, 并返回浮点数</li><li><code>n_trials</code> 整数, 搜索次数, 如果设为 <code>None</code> 将不断搜索直到程序中断</li><li>对于同一个 <code>study</code> 可以执行多次优化任务</li></ul></li><li>属性 <code>best_params</code> 保存字典, 即搜索结束后得到的一组最优的超参数</li><li>方法 <code>trials_dataframe(attrs, multi_index)</code> 将所有搜索结果与参数导出为 Pandas 数据表 <ul><li><code>attrs</code> 元组, 包含需要导出的超参数名</li><li><code>multi_index</code> 布尔值, 是否使用复合索引</li></ul></li></ul><p>如果需要输出优化过程日志, 可使用以下示例代码</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> logging
<span class="token keyword">import</span> sys

<span class="token keyword">import</span> optuna

<span class="token comment"># Add stream handler of stdout to show the messages</span>
optuna<span class="token punctuation">.</span>logging<span class="token punctuation">.</span>get_logger<span class="token punctuation">(</span><span class="token string">&quot;optuna&quot;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>addHandler<span class="token punctuation">(</span>logging<span class="token punctuation">.</span>StreamHandler<span class="token punctuation">(</span>sys<span class="token punctuation">.</span>stdout<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="优化结果可视化" tabindex="-1"><a class="header-anchor" href="#优化结果可视化" aria-hidden="true">#</a> 优化结果可视化</h4>`,8),zn={href:"https://optuna-dashboard.readthedocs.io/en/latest/getting-started.html#installation",target:"_blank",rel:"noopener noreferrer"},qn=t('<p>安装 Optuna 以下扩展模块可实现优化结果的可视化</p><ul><li>可视化核心 <code>pip install optuna-dashboard</code></li><li>加速可视化页面 <code>pip install optuna-fast-fanova gunicorn</code></li></ul><p>通过命令行 <code>optuna-dashboard &lt;数据库地址&gt;</code> 即可加载学习数据进行可视化</p><ul><li><code>数据库地址</code> 参见 <a href="#%E4%BC%98%E5%8C%96%E8%BF%87%E7%A8%8B%E7%AE%A1%E7%90%86">optuna.create_study</a> 方法的 <code>storage</code> 参数</li></ul><h3 id="高层训练框架-pytorch-ignite" tabindex="-1"><a class="header-anchor" href="#高层训练框架-pytorch-ignite" aria-hidden="true">#</a> 高层训练框架 Pytorch Ignite</h3>',5),Cn={href:"https://pytorch-ignite.ai/",target:"_blank",rel:"noopener noreferrer"};function Nn(Ln,Tn){const e=i("ExternalLinkIcon"),o=i("RouterLink");return p(),r("div",null,[u,n("p",null,[s("参考教程 "),n("a",m,[s("https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html"),a(e)])]),h,n("p",null,[s("张量为 Pytorch 中, 一种类似 "),a(o,{to:"/coding/py/math/numpy.html#numpy-%E7%AC%94%E8%AE%B0"},{default:l(()=>[s("Numpy 数组")]),_:1}),s("的数据结构, 用于表示神经模型的输入输出以及作为神经模型的参数"),k,s(" 与 Numpy 数组不同, 张量类型经过专门优化, 可以被 GPU 等硬件加速, 且能够自动记录运算过程并计算梯度, 详见"),b]),v,g,n("ul",null,[n("li",null,[_,s(" 通过序列类型创建张量 "),n("ul",null,[n("li",null,[y,s(" 为元组, 列表等 Python 内置的"),a(o,{to:"/coding/py/base/base.html#%E5%BA%8F%E5%88%97%E7%B1%BB%E5%9E%8B"},{default:l(()=>[s("序列类型")]),_:1})]),f])]),E,x,B,A]),w,n("ul",null,[z,n("li",null,[s("张量访问 "),n("ul",null,[n("li",null,[s("张量的索引与 "),a(o,{to:"/coding/py/math/numpy.html#%E6%95%B0%E7%BB%84%E7%9A%84%E7%B4%A2%E5%BC%95"},{default:l(()=>[s("Numpy 数组的索引")]),_:1}),s("类似")]),q,C])]),n("li",null,[s("张量运算 "),n("ul",null,[N,n("li",null,[s("更多运算操作见 "),n("a",L,[s("https://pytorch.org/docs/stable/tensors.html#torch.Tensor"),a(e)])]),T])])]),W,S,n("ul",null,[D,n("li",null,[s("推荐使用 "),M,s(" 以 "),a(o,{to:"/coding/py/base/base.html#%E9%A2%84%E5%AE%9A%E4%B9%89%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86"},{default:l(()=>[s("with")]),_:1}),s(" 代码块, 在代码块内全局关闭自动梯度功能"),F,s(" 在使用模型进行预测 (正向传播) 时在此代码块内进行以得到更高的运行效率")])]),P,n("p",null,[s("有关全连接层的基础知识参见"),a(o,{to:"/course/machine/computer_vision/ch3.html#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"},{default:l(()=>[s("笔记")]),_:1})]),O,n("ul",null,[H,n("li",null,[s("其他优化器见 "),n("a",I,[s("https://pytorch.org/docs/stable/optim.html"),a(e)]),n("ul",null,[R,n("li",null,[s("对于更多优化器的介绍与使用方法参见 "),n("a",G,[s("https://zhuanlan.zhihu.com/p/416979875"),a(e)])])])])]),U,n("ul",null,[X,n("li",null,[s("损失函数 "),j,n("ul",null,[n("li",null,[s("损失函数定义类位于模块 "),V,s(" 中, 详见 "),n("a",J,[s("https://pytorch.org/docs/stable/nn.html#loss-functions"),a(e)])]),K])])]),Q,n("p",null,[s("关于其中的模型详见 "),n("a",Y,[s("https://pytorch.org/vision/stable/models.html"),a(e)]),Z,s(" 模型的使用方法见"),$]),nn,n("p",null,[s("有关卷积神经网络的基础知识参见"),a(o,{to:"/course/machine/computer_vision/ch3.html#cnn-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"},{default:l(()=>[s("笔记")]),_:1})]),sn,an,n("p",null,[s("参考教程"),en,n("a",tn,[s("https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#preparing-your-data-for-training-with-dataloaders"),a(e)])]),on,n("p",null,[s("关于自定义数据集参见 "),n("a",ln,[s("https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files"),a(e)])]),cn,pn,n("p",null,[s("通过 "),rn,s(" 下与数据集同名的数据集子类创建数据集对象实现数据集的加载"),dn,s(" 更多公开数据集参见 "),n("a",un,[s("https://pytorch.org/vision/stable/datasets.html"),a(e)])]),mn,n("p",null,[n("a",hn,[s("https://github.com/sksq96/pytorch-summary"),a(e)])]),kn,n("ul",null,[n("li",null,[s("参考教程 "),n("ul",null,[n("li",null,[n("a",bn,[s("https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html"),a(e)])]),n("li",null,[n("a",vn,[s("https://zhuanlan.zhihu.com/p/471198169"),a(e)])])])]),n("li",null,[s("参考文档 "),n("a",gn,[s("https://pytorch.org/docs/stable/tensorboard.html"),a(e)])])]),_n,n("p",null,[n("a",yn,[s("https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html"),a(e)])]),fn,n("p",null,[s("参考教程 "),n("a",En,[s("https://optuna.readthedocs.io/en/stable/tutorial/index.html"),a(e)])]),xn,n("p",null,[s("除了超参数, 还可开启剪枝模式, 根据训练过程提前中断没有希望的测试"),Bn,s(" 具体参见 "),n("a",An,[s("https://optuna.readthedocs.io/zh-cn/stable/tutorial/10_key_features/003_efficient_optimization_algorithms.html"),a(e)])]),wn,n("p",null,[s("参考教程 "),n("a",zn,[s("https://optuna-dashboard.readthedocs.io/en/latest/getting-started.html#installation"),a(e)])]),qn,n("p",null,[s("Pytorch Ignite 官方网站 "),n("a",Cn,[s("https://pytorch-ignite.ai/"),a(e)])])])}const Dn=c(d,[["render",Nn],["__file","pytorch.html.vue"]]);export{Dn as default};
